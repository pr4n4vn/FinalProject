[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDAfile by Pranav Nair, ST558",
    "section": "",
    "text": "Introduction\nIn this document, we will do exploratory data analysis of our data set.\nThe first step is to run the libraries necessary.\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe next step is to read in the raw data as an R tibble.\n\ndiabetes_data &lt;- read_csv(\"diabetes_012_health_indicators_BRFSS2015.csv\", col_names = TRUE)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_012, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, He...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(diabetes_data)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_012        : num [1:253680] 0 0 0 0 0 0 0 0 2 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_012 = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nprint(diabetes_data)\n\n# A tibble: 253,680 × 22\n   Diabetes_012 HighBP HighChol CholCheck   BMI Smoker Stroke\n          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1            0      1        1         1    40      1      0\n 2            0      0        0         0    25      1      0\n 3            0      1        1         1    28      0      0\n 4            0      1        0         1    27      0      0\n 5            0      1        1         1    24      0      0\n 6            0      1        1         1    25      1      0\n 7            0      1        0         1    30      1      0\n 8            0      1        1         1    25      1      0\n 9            2      1        1         1    30      1      0\n10            0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nWe see all these variables as numeric. However, many of them should actually be factor variables with meaningful level names.\n\ndiabetes_data_fixed &lt;- diabetes_data |&gt;\n  mutate(Diabetes_012 = factor(Diabetes_012, levels = c(0,1,2), labels = c(\"No Diabetes\", \"Prediabetes\", \"Diabetes\")),\n         HighBP = factor(HighBP, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         Veggies = factor(Veggies, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         AnyHealthcare = factor(AnyHealthcare, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         NoDocbcCost = factor(NoDocbcCost, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex, levels = c(0,1), labels = c(\"Female\", \"Male\")),\n         Age = factor(Age, levels = 1:13, labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n         Education = factor(Education, levels = 1:6, labels = c(\"No Schooling\", \"Elementary\", \"Some High School\", \"High School Grad\", \"Some College\", \"College Grad\")),\n         Income = factor(Income, levels = 1:8, labels = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\", \"$20,000-$25,000\", \"$25,000-$35,000\", \"$35,000-$50,000\", \"$50,000-$75,000\", \"&gt;$75,000\"))\n  )\n\nprint(diabetes_data_fixed)\n\n# A tibble: 253,680 × 22\n   Diabetes_012 HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;        &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No Diabetes  Yes    Yes      Yes          40 Yes    No    \n 2 No Diabetes  No     No       No           25 Yes    No    \n 3 No Diabetes  Yes    Yes      Yes          28 No     No    \n 4 No Diabetes  Yes    No       Yes          27 No     No    \n 5 No Diabetes  Yes    Yes      Yes          24 No     No    \n 6 No Diabetes  Yes    Yes      Yes          25 Yes    No    \n 7 No Diabetes  Yes    No       Yes          30 Yes    No    \n 8 No Diabetes  Yes    Yes      Yes          25 Yes    No    \n 9 Diabetes     Yes    Yes      Yes          30 Yes    No    \n10 No Diabetes  No     No       Yes          24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;\n\n\nNow let’s check the data for missingness.\n\nsum(is.na(diabetes_data_fixed))\n\n[1] 0\n\n\nNo missing data! Now we can move on to the summarizations.\n\n#Showing summary statistics here.\nsummary(diabetes_data_fixed)\n\n      Diabetes_012    HighBP       HighChol     CholCheck         BMI       \n No Diabetes:213703   No :144851   No :146089   No :  9470   Min.   :12.00  \n Prediabetes:  4631   Yes:108829   Yes:107591   Yes:244210   1st Qu.:24.00  \n Diabetes   : 35346                                          Median :27.00  \n                                                             Mean   :28.38  \n                                                             3rd Qu.:31.00  \n                                                             Max.   :98.00  \n                                                                            \n Smoker       Stroke       HeartDiseaseorAttack PhysActivity Fruits      \n No :141257   No :243388   No :229787           No : 61760   No : 92782  \n Yes:112423   Yes: 10292   Yes: 23893           Yes:191920   Yes:160898  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Veggies      HvyAlcoholConsump AnyHealthcare NoDocbcCost     GenHlth     \n No : 47839   No :239424        No : 12417    No :232326   Min.   :1.000  \n Yes:205841   Yes: 14256        Yes:241263    Yes: 21354   1st Qu.:2.000  \n                                                           Median :2.000  \n                                                           Mean   :2.511  \n                                                           3rd Qu.:3.000  \n                                                           Max.   :5.000  \n                                                                          \n    MentHlth         PhysHlth      DiffWalk         Sex              Age       \n Min.   : 0.000   Min.   : 0.000   No :211005   Female:141974   60-64  :33244  \n 1st Qu.: 0.000   1st Qu.: 0.000   Yes: 42675   Male  :111706   65-69  :32194  \n Median : 0.000   Median : 0.000                                55-59  :30832  \n Mean   : 3.185   Mean   : 4.242                                50-54  :26314  \n 3rd Qu.: 2.000   3rd Qu.: 3.000                                70-74  :23533  \n Max.   :30.000   Max.   :30.000                                45-49  :19819  \n                                                                (Other):87744  \n            Education                  Income     \n No Schooling    :   174   &gt;$75,000       :90385  \n Elementary      :  4043   $50,000-$75,000:43219  \n Some High School:  9478   $35,000-$50,000:36470  \n High School Grad: 62750   $25,000-$35,000:25883  \n Some College    : 69910   $20,000-$25,000:20135  \n College Grad    :107325   $15,000-$20,000:15994  \n                           (Other)        :21594  \n\n\nNow, we can move on to the visualization plots.\n\n#Showing visualization plots\n#Distribution of diabetes status\nlibrary(ggplot2)\nggplot(diabetes_data_fixed, aes(x = Diabetes_012)) + \n  geom_bar() + \n  ggtitle(\"Distributions of Diabetes Status\")\n\n\n\n\n\n\n\n\nAs shown in the graph, it looks like most of the participants (over 200,000) in the study do not have diabetes. There are approximately 35,000 people who do have diabnetes, and about 5,000 people who have prediabetes.\n\n#BMI by Diabetes\nggplot(diabetes_data_fixed, aes(x = Diabetes_012, y = BMI)) + \n  geom_boxplot() + \n  ggtitle(\"BMI by Diabetes Status\")\n\n\n\n\n\n\n\n\nThe IQR range of the BMI of people with diabetes is higher than the IQR range of the BMI of people without diabetes. Based off of this, it can be suggested that the BMI is higher for people with diabetes versus people who do not have diabetes.\n\n#Age distribution by Diabetes Status\nggplot(diabetes_data_fixed, aes(x = Age, fill = Diabetes_012)) + \n  geom_bar(position = \"fill\") + \n  ggtitle(\"Age Distribution by Diabetes Status\")\n\n\n\n\n\n\n\n\nAs shown in this stacked bar plot, the distribution looks relatively normal but skewed to the left as the age of participants increased. This suggests that the count of people with diabetes is connected to the age of participants; as the age increased, the likelihood of diabetes also increases.\n\n#Bar plot of physical activity by Diabetes status\nggplot(diabetes_data_fixed, aes(x = PhysActivity, fill = Diabetes_012)) +\n  geom_bar(position = \"fill\") + \n  ggtitle(\"Physical Activity by Diabetes Status\")\n\n\n\n\n\n\n\n\nThis stacked bar plot shows the physical activity versus the count of diabetes. As shown, the count of diabetes is higher for people who do not participate in physical activity, and the count of diabetes is lower for people who do participate in physical activity.\nBelow is a link to the model file.\nLink to my Modeling"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling by Pranav Nair, ST558",
    "section": "",
    "text": "Introduction\nThis data set shows health indicators that could show diabetes. There are 22 variables and 253,680 rows of data. The purpose of this is to investigate the connection between diabetes and health indicators. The goal here is to create some models to predict the Diabetes_binary variable. I will use log loss with 5 fold cross-validation to select the best model.\nLog loss is a performance metric that evaluates the classification model performance by looking at the differences between predicted and actual probabilities. This metric is evaluated by a scale from 0 to 1, with 1 being that the predicted probability is right on spot with the actual probability, and 0 being there is no similarity between the predicted probability and actual probability. Log loss also takes into account the prediction uncertainties, which is why it’s better than accuracy. With log loss on binary classifications, confident wrong predictions are detected and penalized confident wrong predictions.\nFirstly, however, I will need to read in the data once more. The only difference here is that I should create a new diabetes_binary variable which is based off of the diabetes_012 variable.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\nlibrary(readr)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\ndata &lt;- read_csv(\"diabetes_012_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_012, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, He...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata &lt;- data %&gt;%\n  mutate(Diabetes_binary = factor(ifelse(Diabetes_012 == 2, \"Yes\", \"No\")),  # Create binary diabetes variable\n         HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         MentHlth = as.numeric(MentHlth),\n         PhysHlth = as.numeric(PhysHlth),\n         GenHlth = factor(GenHlth, levels = 1:5, labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")),\n         DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n         Age = factor(Age, levels = 1:13, labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n         Education = factor(Education, levels = 1:6, labels = c(\"No Schooling\", \"Elementary\", \"Some High School\", \"High School Grad\", \"Some College\", \"College Grad\")),\n         Income = factor(Income, levels = 1:8, labels = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\", \"$20,000-$25,000\", \"$25,000-$35,000\", \"$35,000-$50,000\", \"$50,000-$75,000\", \"&gt;$75,000\"))\n  )\n\nAs before, I should check for missing values.\n\nsum(is.na(data))\n\n[1] 0\n\n\nNow, I should remove the original diabetes_012 variable.\n\ndata &lt;- data |&gt;\n  select(-Diabetes_012)\n\nNow I will split the data between training data set and test data set.\n\nset.seed(123)\n\ntrainIndex &lt;- createDataPartition(data$Diabetes_binary, p = 0.7,\n                                  list = FALSE,\n                                  times = 1)\n\nTrain_data &lt;- data[ trainIndex,]\nTest_data &lt;- data[-trainIndex,]\n\n\nlogLoss &lt;- function(pred, obs) {\n  -mean(ifelse(obs == 1, log(pred), log(1 - pred)))\n}\n\nThe next step is to create three models. The models I will create are logistic regression, classification tree, and random forest. Logistic regression is a method used for binary classification, where it predicts the probability that an input belongs to a class by fitting a logistic function to the input features. Logistic regression is useful when there is a linear relationship between the input and output. The output of a logistic regression model is mapped from 0 to 1 which shows the likelihood of the input belonging to the positive class.\nNow, I will fit three candidate logistic regression models and choose the best one using CV with log-loss as my metric.\n\n#First, define training control \n\ntrain_control &lt;- trainControl(method = \"cv\", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)\n\nset.seed(123)\n\n\n\nmodel_glm1 &lt;- train(Diabetes_binary ~ ., \n                    data = Train_data, \n                    method = \"glm\", \n                    family = binomial,\n                    trControl = train_control,\n                    metric = \"logLoss\")\n\nmodel_glm2 &lt;- train(Diabetes_binary ~ BMI + HighBP + HighChol + Smoker + Age, \n                    data = Train_data, \n                    method = \"glm\", \n                    family = binomial, \n                    trControl = train_control, \n                    metric = \"logLoss\")\n\nmodel_glm3 &lt;- train(Diabetes_binary ~ BMI * HighBP + HighChol * Smoker + Age,\n                      data = Train_data,\n                      method = \"glm\",\n                      family = binomial,\n                      trControl = train_control,\n                      metric = \"logLoss\")\n\n#Now, we should compare the models\ncompareglm &lt;- resamples(list(glm1 = model_glm1, glm2 = model_glm2, glm3 = model_glm3))\nsummary(compareglm)\n\n\nCall:\nsummary.resamples(object = compareglm)\n\nModels: glm1, glm2, glm3 \nNumber of resamples: 5 \n\nlogLoss \n          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nglm1 0.3154445 0.3184245 0.3185334 0.3184016 0.3189691 0.3206366    0\nglm2 0.3399974 0.3401599 0.3402149 0.3405823 0.3407511 0.3417884    0\nglm3 0.3399582 0.3403277 0.3404895 0.3405504 0.3407590 0.3412176    0\n\n\nBased on what I see here, glm1 is the best model as it has the lowest logLoss value.\n\nbest_glm &lt;- model_glm1\n\nNext, I will create the classification tree model\n\nset.seed(123)\nclass_tree &lt;- train(Diabetes_binary ~ ., \n                    data = Train_data, \n                    method = \"rpart\", \n                    trControl = train_control, \n                    metric = \"logLoss\", \n                    tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))\n\nNext, I will create the random forest model.\n\nset.seed(123)\n\ntrain_control_randomforest &lt;- trainControl(method = \"cv\", number = 3, summaryFunction = mnLogLoss, classProbs = TRUE)\nrandom_forest &lt;- train(Diabetes_binary ~., \n                    data = Train_data,\n                    method = \"ranger\",\n                    trControl = train_control_randomforest,\n                    metric = \"logLoss\",\n                    tuneLength = 3)\n\nGrowing trees.. Progress: 22%. Estimated remaining time: 1 minute, 48 seconds.\nGrowing trees.. Progress: 49%. Estimated remaining time: 1 minute, 5 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 27 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 3 minutes, 57 seconds.\nGrowing trees.. Progress: 27%. Estimated remaining time: 3 minutes, 2 seconds.\nGrowing trees.. Progress: 41%. Estimated remaining time: 2 minutes, 21 seconds.\nGrowing trees.. Progress: 55%. Estimated remaining time: 1 minute, 43 seconds.\nGrowing trees.. Progress: 69%. Estimated remaining time: 1 minute, 11 seconds.\nGrowing trees.. Progress: 79%. Estimated remaining time: 50 seconds.\nGrowing trees.. Progress: 91%. Estimated remaining time: 21 seconds.\nGrowing trees.. Progress: 24%. Estimated remaining time: 1 minute, 40 seconds.\nGrowing trees.. Progress: 49%. Estimated remaining time: 1 minute, 4 seconds.\nGrowing trees.. Progress: 75%. Estimated remaining time: 32 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 3 minutes, 50 seconds.\nGrowing trees.. Progress: 24%. Estimated remaining time: 3 minutes, 20 seconds.\nGrowing trees.. Progress: 36%. Estimated remaining time: 2 minutes, 49 seconds.\nGrowing trees.. Progress: 48%. Estimated remaining time: 2 minutes, 19 seconds.\nGrowing trees.. Progress: 60%. Estimated remaining time: 1 minute, 47 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 1 minute, 14 seconds.\nGrowing trees.. Progress: 84%. Estimated remaining time: 42 seconds.\nGrowing trees.. Progress: 95%. Estimated remaining time: 13 seconds.\nGrowing trees.. Progress: 24%. Estimated remaining time: 1 minute, 40 seconds.\nGrowing trees.. Progress: 50%. Estimated remaining time: 1 minute, 3 seconds.\nGrowing trees.. Progress: 75%. Estimated remaining time: 32 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 3 minutes, 57 seconds.\nGrowing trees.. Progress: 26%. Estimated remaining time: 3 minutes, 1 seconds.\nGrowing trees.. Progress: 41%. Estimated remaining time: 2 minutes, 22 seconds.\nGrowing trees.. Progress: 55%. Estimated remaining time: 1 minute, 47 seconds.\nGrowing trees.. Progress: 67%. Estimated remaining time: 1 minute, 20 seconds.\nGrowing trees.. Progress: 79%. Estimated remaining time: 50 seconds.\nGrowing trees.. Progress: 93%. Estimated remaining time: 17 seconds.\nGrowing trees.. Progress: 23%. Estimated remaining time: 1 minute, 44 seconds.\nGrowing trees.. Progress: 48%. Estimated remaining time: 1 minute, 6 seconds.\nGrowing trees.. Progress: 75%. Estimated remaining time: 32 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 3 minutes, 57 seconds.\nGrowing trees.. Progress: 27%. Estimated remaining time: 2 minutes, 59 seconds.\nGrowing trees.. Progress: 41%. Estimated remaining time: 2 minutes, 19 seconds.\nGrowing trees.. Progress: 55%. Estimated remaining time: 1 minute, 43 seconds.\nGrowing trees.. Progress: 69%. Estimated remaining time: 1 minute, 11 seconds.\nGrowing trees.. Progress: 83%. Estimated remaining time: 39 seconds.\nGrowing trees.. Progress: 97%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 24%. Estimated remaining time: 1 minute, 43 seconds.\nGrowing trees.. Progress: 51%. Estimated remaining time: 1 minute, 3 seconds.\nGrowing trees.. Progress: 77%. Estimated remaining time: 28 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 3 minutes, 50 seconds.\nGrowing trees.. Progress: 27%. Estimated remaining time: 2 minutes, 59 seconds.\nGrowing trees.. Progress: 41%. Estimated remaining time: 2 minutes, 19 seconds.\nGrowing trees.. Progress: 55%. Estimated remaining time: 1 minute, 43 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 1 minute, 10 seconds.\nGrowing trees.. Progress: 84%. Estimated remaining time: 36 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 3 seconds.\nGrowing trees.. Progress: 23%. Estimated remaining time: 1 minute, 44 seconds.\nGrowing trees.. Progress: 48%. Estimated remaining time: 1 minute, 7 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 35 seconds.\nGrowing trees.. Progress: 94%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 12%. Estimated remaining time: 4 minutes, 4 seconds.\nGrowing trees.. Progress: 27%. Estimated remaining time: 3 minutes, 7 seconds.\nGrowing trees.. Progress: 41%. Estimated remaining time: 2 minutes, 25 seconds.\nGrowing trees.. Progress: 55%. Estimated remaining time: 1 minute, 48 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 1 minute, 12 seconds.\nGrowing trees.. Progress: 84%. Estimated remaining time: 37 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 3 seconds.\n\n\n\nprint(class_tree$bestTune)\n\n     cp\n2 0.001\n\nprint(random_forest$bestTune)\n\n  mtry splitrule min.node.size\n1    2      gini             1\n\n\nFinal Model Selection - Here, we will determine which model is the best model.\n\nglm_predictions &lt;- predict(best_glm, Test_data, type = \"prob\")[,2]\ntree_predictions &lt;- predict(class_tree, Test_data, type = \"prob\")[,2]\nrandomforest_predictions &lt;- predict(random_forest, Test_data, type = \"prob\")[,2]\n\nCalculate and print log loss on each model with the test data set.\n\nlogLoss_glm &lt;- -mean(log(glm_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))\nlogLoss_class &lt;- -mean(log(tree_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))\nlogLoss_rf &lt;- -mean(log(randomforest_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))\n\nprint(paste(\"Log loss for logistic regression:\", logLoss_glm))\n\n[1] \"Log loss for logistic regression: 3.2054672251223\"\n\nprint(paste(\"Log loss for classification tree:\", logLoss_class))\n\n[1] \"Log loss for classification tree: 2.43845939602353\"\n\nprint(paste(\"Log loss for random forest:\", logLoss_rf))\n\n[1] \"Log loss for random forest: 2.5831410509699\"\n\n\nNow I will show which is the best model\n\nnames_models &lt;- c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\")\nlog_losses &lt;- c(logLoss_glm, logLoss_class, logLoss_rf)\nmodel_index &lt;- which.min(log_losses)\nbest_model &lt;- names_models[model_index]\nprint(paste(\"The overall best model is:\", best_model))\n\n[1] \"The overall best model is: Classification Tree\"\n\nmodel_best &lt;- class_tree\n\n# model_best &lt;- ifelse(logLoss_glm &lt; logLoss_class & logLoss_glm &lt; logLoss_rf, model_glm, ifelse(log_loss_tree &lt; log_loss_rf, model_tree, model_rf))\n\nFrom what we see here, the logistic regression model is the best model with the least log loss. Now, let’s save the best model into a file.\n\nsaveRDS(model_best, \"best_model.rds\")"
  }
]