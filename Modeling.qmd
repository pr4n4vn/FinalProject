---
title: "Modeling by Pranav Nair, ST558"
format: html
editor: visual
---

### Introduction

This data set shows health indicators that could show diabetes. There are 22 variables and 253,680 rows of data. The purpose of this is to investigate the connection between diabetes and health indicators. The goal here is to create some models to predict the Diabetes_binary variable. I will use log loss with 5 fold cross-validation to select the best model.

Log loss is a performance metric that evaluates the classification model performance by looking at the differences between predicted and actual probabilities. This metric is evaluated by a scale from 0 to 1, with 1 being that the predicted probability is right on spot with the actual probability, and 0 being there is no similarity between the predicted probability and actual probability. Log loss also takes into account the prediction uncertainties, which is why it's better than accuracy. With log loss on binary classifications, confident wrong predictions are detected and penalized confident wrong predictions.

Firstly, however, I will need to read in the data once more. The only difference here is that I should create a new diabetes_binary variable which is based off of the diabetes_012 variable.

```{r}
library(caret)
library(tidyverse)
library(e1071)
library(readr)
library(randomForest)


data <- read_csv("diabetes_012_health_indicators_BRFSS2015.csv")

data <- data %>%
  mutate(Diabetes_binary = factor(ifelse(Diabetes_012 == 2, "Yes", "No")),  # Create binary diabetes variable
         HighBP = factor(HighBP, levels = c(0, 1), labels = c("No", "Yes")),
         HighChol = factor(HighChol, levels = c(0, 1), labels = c("No", "Yes")),
         CholCheck = factor(CholCheck, levels = c(0, 1), labels = c("No", "Yes")),
         Smoker = factor(Smoker, levels = c(0, 1), labels = c("No", "Yes")),
         Stroke = factor(Stroke, levels = c(0, 1), labels = c("No", "Yes")),
         HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c("No", "Yes")),
         PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("No", "Yes")),
         Fruits = factor(Fruits, levels = c(0, 1), labels = c("No", "Yes")),
         Veggies = factor(Veggies, levels = c(0, 1), labels = c("No", "Yes")),
         HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("No", "Yes")),
         AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c("No", "Yes")),
         NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c("No", "Yes")),
         MentHlth = as.numeric(MentHlth),
         PhysHlth = as.numeric(PhysHlth),
         GenHlth = factor(GenHlth, levels = 1:5, labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
         DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c("No", "Yes")),
         Sex = factor(Sex, levels = c(0, 1), labels = c("Female", "Male")),
         Age = factor(Age, levels = 1:13, labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80+")),
         Education = factor(Education, levels = 1:6, labels = c("No Schooling", "Elementary", "Some High School", "High School Grad", "Some College", "College Grad")),
         Income = factor(Income, levels = 1:8, labels = c("<$10,000", "$10,000-$15,000", "$15,000-$20,000", "$20,000-$25,000", "$25,000-$35,000", "$35,000-$50,000", "$50,000-$75,000", ">$75,000"))
  )

```

As before, I should check for missing values.

```{r}
sum(is.na(data))
```

Now, I should remove the original diabetes_012 variable.

```{r}
data <- data |>
  select(-Diabetes_012)
```

Now I will split the data between training data set and test data set.

```{r}
set.seed(123)

trainIndex <- createDataPartition(data$Diabetes_binary, p = 0.7,
                                  list = FALSE,
                                  times = 1)

Train_data <- data[ trainIndex,]
Test_data <- data[-trainIndex,]


logLoss <- function(pred, obs) {
  -mean(ifelse(obs == 1, log(pred), log(1 - pred)))
}
```

The next step is to create three models. The models I will create are logistic regression, classification tree, and random forest. Logistic regression is a method used for binary classification, where it predicts the probability that an input belongs to a class by fitting a logistic function to the input features. Logistic regression is useful when there is a linear relationship between the input and output. The output of a logistic regression model is mapped from 0 to 1 which shows the likelihood of the input belonging to the positive class.

Now, I will fit three candidate logistic regression models and choose the best one using CV with log-loss as my metric.

```{r}

#First, define training control 

train_control <- trainControl(method = "cv", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)

set.seed(123)



model_glm1 <- train(Diabetes_binary ~ ., 
                    data = Train_data, 
                    method = "glm", 
                    family = binomial,
                    trControl = train_control,
                    metric = "logLoss")

model_glm2 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + Smoker + Age, 
                    data = Train_data, 
                    method = "glm", 
                    family = binomial, 
                    trControl = train_control, 
                    metric = "logLoss")

model_glm3 <- train(Diabetes_binary ~ BMI * HighBP + HighChol * Smoker + Age,
                      data = Train_data,
                      method = "glm",
                      family = binomial,
                      trControl = train_control,
                      metric = "logLoss")

#Now, we should compare the models
compareglm <- resamples(list(glm1 = model_glm1, glm2 = model_glm2, glm3 = model_glm3))
summary(compareglm)

```

Based on what I see here, glm1 is the best model as it has the lowest logLoss value.

```{r}
best_glm <- model_glm1
```

Next, I will create the classification tree model

```{r}
set.seed(123)
class_tree <- train(Diabetes_binary ~ ., 
                    data = Train_data, 
                    method = "rpart", 
                    trControl = train_control, 
                    metric = "logLoss", 
                    tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))
```

Next, I will create the random forest model.

```{r}
set.seed(123)

train_control_randomforest <- trainControl(method = "cv", number = 3, summaryFunction = mnLogLoss, classProbs = TRUE)
random_forest <- train(Diabetes_binary ~., 
                    data = Train_data,
                    method = "ranger",
                    trControl = train_control_randomforest,
                    metric = "logLoss",
                    tuneLength = 3)

```

```{r}
print(class_tree$bestTune)
print(random_forest$bestTune)
```

Final Model Selection - Here, we will determine which model is the best model.

```{r}
glm_predictions <- predict(best_glm, Test_data, type = "prob")[,2]
tree_predictions <- predict(class_tree, Test_data, type = "prob")[,2]
randomforest_predictions <- predict(random_forest, Test_data, type = "prob")[,2]


```

Calculate and print log loss on each model with the test data set.

```{r}
logLoss_glm <- -mean(log(glm_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))
logLoss_class <- -mean(log(tree_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))
logLoss_rf <- -mean(log(randomforest_predictions[cbind(1:nrow(Test_data), as.numeric(Test_data$Diabetes_binary))]))

print(paste("Log loss for logistic regression:", logLoss_glm))
print(paste("Log loss for classification tree:", logLoss_class))
print(paste("Log loss for random forest:", logLoss_rf))
```

Now I will show which is the best model

```{r}
names_models <- c("Logistic Regression", "Classification Tree", "Random Forest")
log_losses <- c(logLoss_glm, logLoss_class, logLoss_rf)
model_index <- which.min(log_losses)
best_model <- names_models[model_index]
print(paste("The overall best model is:", best_model))

model_best <- class_tree

# model_best <- ifelse(logLoss_glm < logLoss_class & logLoss_glm < logLoss_rf, model_glm, ifelse(log_loss_tree < log_loss_rf, model_tree, model_rf))
```

From what we see here, the logistic regression model is the best model with the least log loss. Now, let's save the best model into a file.

```{r}
saveRDS(model_best, "best_model.rds")
```
